# pokemon_nn_on_steroids

![Python](https://img.shields.io/badge/python-3.9-blue?logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.8-EE4C2C?logo=pytorch&logoColor=white)
![Lightning](https://img.shields.io/badge/Lightning-2.6-792EE5?logo=lightning&logoColor=white)
![HuggingFace](https://img.shields.io/badge/ğŸ¤—%20HuggingFace-yellow)
![Docker](https://img.shields.io/badge/Docker-ready-2496ED?logo=docker&logoColor=white)

## Description 
Ce projet est plutot court, il reprend des Ã©lÃ©ments du projet pokemon_nn qui avait Ã©tÃ© rÃ©alisÃ© dans un cadre scolaire et le pousse a un niveau plus professionnel.  
Il s'agissait d'un projet de computer vision qui entrainait un classifieur a reconnaitre les pokemons de la premiÃ¨re gÃ©nÃ©ration.  
Cependant il s'agissait uniquement d'un projet acadÃ©mique, tout reposait dans un notebook ce qui est trÃ¨s bien pour de l'explo ou du scolaire mais niveau mise en prod ...  
Donc j'en ai profitÃ© pour le refondre totalement en en proposant une version "sous stÃ©roides".  
Le premier projet Ã©tait un rÃ©seau de neuronne convolutif (CNN) trÃ¨s lÃ©ger qui pouvait tourner en local et donc on a pensÃ© l'architecture afin qu'elle soit lÃ©gÃ¨re.  
Cette fois il s'agit de ce qui se fait de mieux actuellement en termes de computer vision un VIT (Vision Transformer).  
Encore une fois j'ai adoptÃ© la stratÃ©gie du transfer learning afin d'avoir d'excellents rÃ©sultats en capitalisant sur l'entrainement d'autrui.  
Pour rÃ©sumer : ***on prend le squelette de pokemon_nn et on l'habille chez Gucci***

## DÃ©monstration
Petite dÃ©monstration du modÃ¨le sur des images du test set.  

<p align="center">
  <a href="https://youtu.be/7ZLl6D2uANc">
    <img src="https://img.youtube.com/vi/7ZLl6D2uANc/0.jpg" alt="DÃ©mo pokemon_nn_on_steroids" width="600">
  </a>
</p>

## Principales diffÃ©rences avec pokemon_nn v1 

### Frameworks 
La premiere version de ce projet Ã©tait en Keras (built au dessus de Tensorflow).  
Btw j'ai dÃ©couvert rÃ©cemment qu'on peut set le backend de Keras pour pytorch.  
Cette version amÃ©liorÃ©e et production-ready du projet [pokemon_nn](https://github.com/gaabsi/pokemon_nn/tree/main) est en pytorch_lightning (built au dessus de Pytorch qui permet d'Ãªtre bcp moins verbeux et de garder une structure qui varie peu (top pour debug)).  

### ModÃ¨les
Pour le premier projet, on s'Ã©tait fixÃ© comme contrainte l'efficience, on voulait un projet plutot lÃ©ger qui puisse Ãªtre entrainÃ© sur un laptop et le challenge c'Ã©tait donc d'en faire un dont l'architecture compensait (en partie) les faibles ressources machines qu'on pouvait lui allouer.  
On a rÃ©ussi ce premier challenge en utilisant un CNN.  
Mais avec plus de puissance de calcul sous le coude (merci les VM distantes) il devient possible de faire un modÃ¨le vraiment super performants en partant des mÃªmes donnÃ©es.  
Donc pour cette version sous stÃ©roÃ®des plus de contraintes machines je voulais juste un modele qui prend les mÃªmes images d'entrainement et voir ce que ca fait aux performances.  
En classification d'image ce qui se fait de mieux c'est les ViT, je suis parti sur un ViT particulier qui apprend particulierement bien des petits datasets DeiT, une version dÃ©veloppÃ©e par facebook et j'ai lancÃ© un entrainement rapide en dÃ©-freezant la tÃªte de classification et en l'appliquant Ã  ma tÃ¢che de classification.  

### PrÃ©-processing 
C'est le seul point qui diverge un peu des deux mÃ©thodologies, dans la version originale de pokemon_nn, pour avoir un modÃ¨le super lÃ©ger on l'avait fait tourner sur des toutes petites images (64x64) lÃ  les images sont toujours relativement petites, mais comparativement elles sont beaucoup plus grosses car on donne a manger Ã  notre DeiT des images de taille (224x224).  
Donc j'avais 2 choix sur les images qui Ã©taient plus petites que (224x224) : 
- les supprimer : dommage on perd de la donnÃ©e
- les augmenter : on peut avoir des images un peu farfelues et si en plus on les augmente on peut crÃ©er des monstres
J'aurais aussi pu 0 padder autour mais c'est pas l'idÃ©al. 

Donc j'ai choisi l'option de les supprimer et on a donc un peu moins de data que dans la premiere version du projet.  
Ensuite on fait la pipeline classique : 
- split en train test et val 
- augmentation sur le train set pour rÃ©Ã©quilibrer les classes 
- ***Dans cet ordre lÃ  pour Ã©viter le leakage*** 
- ... training ... 
- ... evaluation ... 

### Performances 
Comparons rapidement les performances de nos 2 modÃ¨les.  

![old_modele](models/old/old_eval_modele.png)
Ce premier modele (version CNN lÃ©ger Tensorflow) on avait tout fait pour le limiter mais on avait quand meme un petit overfit de notre modÃ¨le.  
Le modÃ¨le n'Ã©tais pas mauvais mais n'excellait pas non plus on avait une val_acc autour des 0.7 ce qui encore une fois est plutot pas mauvais compte tenu des limites qu'on s'Ã©tait fixÃ© (lÃ©gÃ¨retÃ© de l'entrainement du modÃ¨le + nombre de classes, ...).  
***On avait une accuracy sur le jeu de test de 71.21%.  ***
Passons maintenant Ã  la star du show, voici les performances du deuxieme modÃ¨le.  

![new_model_ver_0](models/version_0/lightning_logs/eval_modele.png)

Ici on est sur le Data efficient image Transformer (DeiT) et on voit qu'on a des performances bien plus Ã©levÃ©es.  
On a toujours un petit overfit mais qui est bien plus rÃ©duit que sur le premier modÃ¨le.  
Les performances sont nettement supÃ©rieures Ã  celles de notre CNN lÃ©ger (Ã§a s'explique par la meilleure qualitÃ© des images mais aussi car on compare une bombe atomique Ã  un bÃ©bÃ© qui tousse en terme de nombre de paramÃ¨tres et de complexitÃ©).  
J'aurais pu avoir des performances encore un peu meilleures en dÃ©-freezant plus que la tÃªte de classification mais honnÃªtement les performances Ã©taient dÃ©jÃ  tellement bonnes que je me suis arrÃªtÃ© lÃ .  
***En utilisant notre nouveau modÃ¨le pour prÃ©dire les labels de notre test set on a une accuracy de 91.77%***

![modele_olf_vs_new](https://preview.redd.it/how-many-coughing-babies-does-it-take-to-defeat-one-v0-lj14t45ul16d1.jpeg?width=640&crop=smart&auto=webp&s=d7bdf455b10397b74f5489259b5a10b6a6e4b448)

Les deux moÃ¨les ont leurs avantages et leurs inconvÃ©nients (majoritairement au niveau de la puissance de calcul nÃ©cessaire pour l'entrainement) mais en termes de performances le second l'emporte haut la main.  

## Structure du projet 
```
pokemon_nn_on_steroids/ 
    â”œâ”€â”€ README.md 
    â”œâ”€â”€ src/                            # Codes du projet
    â”‚   â”œâ”€â”€ get_images.py               # Script qui prend le dataset zipÃ© que j'ai stockÃ© sur mon Huggingface
    â”‚   â”œâ”€â”€ data_prep.py                # Script de data prÃ©paration 
    â”‚   â”œâ”€â”€ modele.py                   # Modelisation et entrainement
    â”‚   â”œâ”€â”€ evaluation.py               # Evaluation des performances du modele
    â”‚   â””â”€â”€ app.py                      # Streamlit ultra basique pour me la peter dans le README
    â”œâ”€â”€ models/                         # Stockage des modeles entrainÃ©s
    â”‚   â”œâ”€â”€ old/                        # Ancienne version du modele (CNN)
    â”‚   â”‚   â”œâ”€â”€ old_eval_modele.png     # Ancien schÃ©ma pour monitorer l'over/under fit du modele
    â”‚   â”‚   â””â”€â”€ pokedex_64x64.keras     # Poids du modÃ¨le (CNN) de la v1 du projet 
    â”‚   â””â”€â”€ version_0/                  # Premiere (et unique) version du nouveau modele
    â”‚       â”œâ”€â”€ version_0.ckpt          # Poids de notre premier modele (pas push mais sur Hugging Face Hub on le rÃ©cupÃ¨re juste aprÃ¨s)
    â”‚       â””â”€â”€ lightning_logs/         # Logs de l'entrainement du modele
    â”‚           â”œâ”€â”€ eval_modele.png     # SchÃ©mas pour monitorer l'eventuel l'over/under fit du modele
    â”‚           â”œâ”€â”€ metrics.csv         # Logs gÃ©nÃ©rÃ©s par lightning
    â”‚           â””â”€â”€ test_perf.csv       # PrÃ©dictions dÃ©taillÃ©es (chaque pred sur le test set, sa proba, son true label, ...)
    â”œâ”€â”€ data/                           # Data dir (non push) 
    â”‚   â”œâ”€â”€ raw_data/                   # Dataset unzipÃ© qui sort tout fraichement de mon Huggingface
    â”‚   â””â”€â”€  split_data/                 # Dir qui contient les train, val et test data splitÃ©s proprement 
    â”‚       â”œâ”€â”€ train/                  # Images utilisÃ©es pour le training
    â”‚       â”œâ”€â”€ val/                    # Images utilisÃ©es pour la validation
    â”‚       â””â”€â”€ test/                   # Images utilisÃ©es pour le test
    â”œâ”€â”€ Dockerfile                      # Dockerfile du projet
    â”œâ”€â”€ main.sh                         # Pour lancer tout le projet sans se casser la tete (entrainement gourmand, dÃ©conseillÃ© en local, prÃ©ferer une VM)
    â”œâ”€â”€ .gitignore                      # Fichiers a ignorer par Git 
    â””â”€â”€ requirements.txt                # Packages du projet


```

## Clonage du projet 
Pour reprendre le projet, l'idÃ©al c'est de pas refaire tourner tout l'entrainement et de rÃ©cupÃ©rer uniquement les poids pour load le modele.  
Moi pour l'entrainement je n'ai pas voulu faire bruler mon pc j'ai donc fait tourner ca dans un pod de runpod, sur docker_hub sous le mÃªme nom que ce projet et tag :latest il y a l'image que j'avais utilisÃ© pour explorer un peu l'entrainement du modÃ¨le.  

Pour rÃ©utiliser le projet : 
```bash 
cd ~
git clone https://github.com/gaabsi/pokemon_nn_on_steroids.git
```

On rÃ©cupÃ¨re les poids que j'ai stockÃ© sur Huggingface (le dataset aussi si on regarde bien les codes lol) : 
```bash 
cd ~/pokemon_nn_on_steroids
huggingface-cli download gaabsi/pokemon_nn_on_steroids version_0.ckpt --local-dir ./models/version_0
rm -rf ./models/version_0/.cache/
```

On crÃ©e l'image Docker et on run :
```bash 
cd ~/pokemon_nn_on_steroids
docker buildx build --platform linux/amd64,linux/arm64 -t pokemon_nn_on_steroids:latest . 
docker run --rm -v ~/pokemon_nn_on_steroids:/app pokemon_nn_on_steroids:latest 
```

Par dÃ©faut dans le main qui est exÃ©cutÃ© j'ai commentÃ© la ligne d'entrainement et comme on a importÃ© les poids depuis Huggingface Ã§a fonctionne mais c'est possible de la dÃ©commenter si vous avez le GPU adaptÃ©.  

